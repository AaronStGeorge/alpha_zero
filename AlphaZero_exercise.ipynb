{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "AlphaZero_exercise.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xST_HD4EI-uL",
        "colab_type": "text"
      },
      "source": [
        "# AlphaGo Zero\n",
        "---\n",
        "For this exercise we will implement AlphaZero which is a more generalized form of AlphaGO and train it on a game of \n",
        "connect 4. We will first play against AlphaZero when it has not trained at all. We will then let it train for a few training \n",
        "cycles and play it again to observe how it has improved. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "kqVyB4PiI-uM",
        "colab_type": "text"
      },
      "source": [
        "## 1. AlphaZero Configuration\n",
        "---\n",
        "\n",
        "First lets make a class to keep all of our hyper parameters in one spot. This has been done for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0ESdWe4NI-uN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "class AlphaZeroConfig(object):\n",
        "    \"\"\"\n",
        "    This holds the configuration parameters\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Self-Play ==\n",
        "        self.num_sampling_moves = 30\n",
        "        self.max_moves = 42\n",
        "        self.num_simulations = 25 # 25\n",
        "\n",
        "        # Root prior exploration noise.\n",
        "        self.root_dirichlet_alpha = 0.3  # for chess, 0.03 for Go and 0.15 for shogi.\n",
        "        self.root_exploration_fraction = 0.25\n",
        "\n",
        "        # UCB formula\n",
        "        self.pb_c_base = 19652\n",
        "        self.pb_c_init = 1.25\n",
        "\n",
        "        # Training ==\n",
        "        self.training_steps = int(9)    # number of times we perform gradient descent\n",
        "        self.window_size = int(20)      # number of games played during self play\n",
        "        self.batch_size = 4000            # size of training batch\n",
        "        self.cycles = 3                 # number of\n",
        "\n",
        "        self.weight_decay = 1e-4\n",
        "        self.momentum = 0.9\n",
        "        self.learning_rate = 1e-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "SNHXdaS0I-uR",
        "colab_type": "text"
      },
      "source": [
        "## 2. Game Definition\n",
        "---\n",
        "Next lets set up the connect 4 game. This part has been done for you. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "K2aARZTnI-uS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy\n",
        "from typing import List\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "  device = \"cuda:0\" \n",
        "else:  \n",
        "  device = \"cpu\" \n",
        "\n",
        "class Node(object):\n",
        "\n",
        "    def __init__(self, prior: float):  # prior = how good the network thought it would be\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "class Game(object):\n",
        "\n",
        "    def __init__(self, history=None):\n",
        "        # Connect 4 specific ===\n",
        "        self._num_rows = 6\n",
        "        self._num_cols = 7\n",
        "\n",
        "        self._winner = None\n",
        "\n",
        "        # Masks to \"convolve\" over board and detect a winner\n",
        "        self._win_masks = []\n",
        "        # Horizontal wins\n",
        "        for i in range(4):\n",
        "            mask = np.zeros((4, 4), dtype=np.bool)\n",
        "            mask[i, :] = True\n",
        "            self._win_masks.append(mask)\n",
        "        # Vertical wins\n",
        "        for j in range(4):\n",
        "            mask = np.zeros((4, 4), dtype=np.bool)\n",
        "            mask[:, j] = True\n",
        "            self._win_masks.append(mask)\n",
        "        # Diagonal wins\n",
        "        down = np.zeros((4, 4), dtype=np.bool)\n",
        "        for i, j in zip(range(4), range(4)):\n",
        "            down[i, j] = True\n",
        "        self._win_masks.append(down)\n",
        "        up = np.zeros((4, 4), dtype=np.bool)\n",
        "        for i, j in zip(reversed(range(4)), range(4)):\n",
        "            up[i, j] = True\n",
        "        self._win_masks.append(up)\n",
        "\n",
        "        # All games will have these ===\n",
        "        self.history = history or []\n",
        "        self.child_visits = []\n",
        "        self.num_actions = self._num_cols  # 7 for connect 4, 512 for chess/shogi, and 722 for Go.\n",
        "\n",
        "    def terminal(self):\n",
        "        \"\"\"\n",
        "        returns bool if the game is finished or not\n",
        "        \"\"\"\n",
        "        if self._winner is not None or len(self.history) == 42:\n",
        "            return True\n",
        "\n",
        "        image = self.make_image(len(self.history))\n",
        "        # check for wins from the bottom of the board up. Wins are more likely to appear there.\n",
        "        for i in reversed(range(self._num_rows - 3)):\n",
        "            for j in range(self._num_cols - 3):\n",
        "                for mask in self._win_masks:\n",
        "                    for player in range(2):\n",
        "                        test = image[player, i:i + 4, j:j + 4][mask]\n",
        "                        if np.alltrue(test == 1):\n",
        "                            self._winner = player\n",
        "                            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def terminal_value(self, to_play):\n",
        "        \"\"\"\n",
        "        The result of the game from the player that's going to_play? If player 1\n",
        "        won then and to_play is 1 then return 1 if to_play is 2 then return -1?\n",
        "        \"\"\"\n",
        "        if self._winner is None and len(self.history) == 42:\n",
        "            return 0\n",
        "        return to_play == self._winner\n",
        "\n",
        "    def legal_actions(self):\n",
        "        image = self.make_image(len(self.history))\n",
        "        return [j for j in range(self._num_cols) if image[0, 0, j] == 0 and image[1, 0, j] == 0]\n",
        "\n",
        "    def clone(self):\n",
        "        return Game(list(self.history))\n",
        "\n",
        "    def apply(self, action: int):\n",
        "        self.history.append(action)\n",
        "\n",
        "    def store_search_statistics(self, root: Node):\n",
        "        sum_visits = sum(child.visit_count for child in iter(root.children.values()))\n",
        "        self.child_visits.append([\n",
        "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "            for a in range(self.num_actions)\n",
        "        ])\n",
        "\n",
        "    def make_image(self, state_index: int):\n",
        "        \"\"\"\n",
        "        returns what the game looked like at state_index i\n",
        "        \"\"\"\n",
        "        player_0 = np.zeros((self._num_rows, self._num_cols), dtype=numpy.float)\n",
        "        player_1 = np.zeros((self._num_rows, self._num_cols), dtype=numpy.float)\n",
        "        for move_i, move in enumerate(self.history[:state_index+1]):\n",
        "            for row in reversed(range(self._num_rows)):\n",
        "                if player_0[row, move] == 0 and player_1[row, move] == 0:\n",
        "                    if move_i % 2 == 0:\n",
        "                        player_0[row, move] = 1\n",
        "                    if move_i % 2 == 1:\n",
        "                        player_1[row, move] = 1\n",
        "                    break\n",
        "\n",
        "        to_play = (state_index + 1) % 2 * np.ones((self._num_rows, self._num_cols), dtype=numpy.float)\n",
        "\n",
        "        return np.array([player_0, player_1, to_play], dtype=numpy.float)\n",
        "\n",
        "    def make_target(self, state_index: int):\n",
        "        \"\"\"\n",
        "        returns the nural network target i.e. what the NN should be gessing given the image\n",
        "        \"\"\"\n",
        "        return (self.terminal_value(state_index % 2),  # state_index % 2 will always be who's playing\n",
        "                self.child_visits[state_index])\n",
        "\n",
        "    def to_play(self):\n",
        "        \"\"\"\n",
        "        Return the player that is about to play\n",
        "        \"\"\"\n",
        "        return len(self.history) % 2\n",
        "\n",
        "    def __str__(self):\n",
        "        board_state = self.make_image(len(self.history))\n",
        "\n",
        "        out = \"\"\n",
        "        for i in range(self._num_rows):\n",
        "            out += f\"{i}|\"\n",
        "            for j in range(self._num_cols):\n",
        "                if board_state[0, i, j] == 1:\n",
        "                    out += \" ○ \"\n",
        "                elif board_state[1, i, j] == 1:\n",
        "                    out += \" ● \"\n",
        "                else:\n",
        "                    out += \"   \"\n",
        "            out += \"|\\n\"\n",
        "\n",
        "        out += \"  \"\n",
        "        for j in range(self._num_cols):\n",
        "            out += f\" \\u0305{j} \"\n",
        "        return out\n",
        "    \n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, config: AlphaZeroConfig):\n",
        "        self.window_size = config.window_size\n",
        "        self.batch_size = config.batch_size\n",
        "        self.buffer = []\n",
        "\n",
        "    def save_game(self, game):\n",
        "        if len(self.buffer) > self.window_size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(game)\n",
        "\n",
        "    def sample_batch(self):\n",
        "        # Sample uniformly across positions.\n",
        "        move_sum = float(sum(len(g.history) for g in self.buffer))\n",
        "        games = numpy.random.choice(\n",
        "            self.buffer,\n",
        "            size=self.batch_size,\n",
        "            p=[len(g.history) / move_sum for g in self.buffer])\n",
        "        game_pos = [(g, numpy.random.randint(len(g.history))) for g in games]\n",
        "\n",
        "        image = np.array([g.make_image(i) for (g, i) in game_pos], dtype=np.float)\n",
        "        image = torch.from_numpy(image)\n",
        "        image = image.to(torch.float)\n",
        "\n",
        "        policy_target = np.array([g.make_target(i)[1] for (g, i) in game_pos])\n",
        "        policy_target = torch.from_numpy(policy_target)\n",
        "        policy_target = policy_target.to(torch.float)\n",
        "\n",
        "        value_target = np.array([g.make_target(i)[0] for (g, i) in game_pos])\n",
        "        value_target = torch.from_numpy(value_target)\n",
        "        value_target = value_target.to(torch.float)\n",
        "\n",
        "        batch_data = TensorDataset(image, policy_target, value_target)\n",
        "        return torch.utils.data.DataLoader(dataset=batch_data,\n",
        "                                           batch_size=50,\n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "efYv8f0bI-uW",
        "colab_type": "text"
      },
      "source": [
        "## 3. One Network Two Heads\n",
        "---\n",
        "Two heads are smarter than one right? Lets implement the tow headed Neural Network. Recall that AlphaGo Zero uses Convolutional \n",
        "ResNet architecture with two heads. One that outputs a probability distribution over all possible moves $(p)$ and another that \n",
        "outputs a single scalar value $(v)$ representing the value of the current state. \n",
        "\n",
        "The neural network is defined as:  \n",
        "$$f_\\theta (s) = (\\mathbf{p,v})$$  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pgV4dCsNI-uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        Convolution Block\n",
        "        \"\"\"\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        ResNet Block\n",
        "        \"\"\"\n",
        "        self.res_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128)\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        Value Head\n",
        "        \"\"\"\n",
        "        self.value_convolv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=3, kernel_size=1),\n",
        "            nn.BatchNorm2d(num_features=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.value_linear = nn.Sequential(\n",
        "            nn.Linear(in_features=126, out_features=32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=32, out_features=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        Policy Head\n",
        "        \"\"\"\n",
        "        self.policy_convolv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=1),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.policy_linear = nn.Sequential(\n",
        "            nn.Linear(6*7*32, 7),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def inference(self, image):\n",
        "        image = torch.from_numpy(image)\n",
        "        image = image.to(torch.float)\n",
        "        image = image.unsqueeze(0)\n",
        "\n",
        "        p, v = self.forward(image)\n",
        "\n",
        "        return float(v.squeeze().detach()), p.squeeze().detach().cpu().numpy()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "\n",
        "        num_blocks = 10\n",
        "\n",
        "        x = x.to(device)\n",
        "        \"\"\"\n",
        "        ResNet\n",
        "        \"\"\"\n",
        "        x = self.conv_block(x)\n",
        "        for i in range(num_blocks):\n",
        "            residual = x\n",
        "            x = self.res_block(x)\n",
        "            x += residual\n",
        "            x = nn.functional.relu(x, inplace=True)\n",
        "\n",
        "        \"\"\"\n",
        "        Value Head\n",
        "        \"\"\"\n",
        "        v = self.value_convolv(x)\n",
        "        v = v.view(-1, 3 * 6 * 7)\n",
        "        v = self.value_linear(v)\n",
        "\n",
        "        \"\"\"\n",
        "        Policy Head\n",
        "        \"\"\"\n",
        "        p = self.policy_convolv(x)\n",
        "        p = p.view(-1, 6 * 7 * 32)\n",
        "        p = self.policy_linear(p)\n",
        "\n",
        "        return p, v\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PbVMq1bbI-ua",
        "colab_type": "text"
      },
      "source": [
        "## 4. The training pipeline\n",
        "---\n",
        "AlphaZero training is split into two independent parts: Network training and self-play data generation.\n",
        "These two parts only communicate by transferring the latest network checkpoint\n",
        "from the training to the self-play, and the finished games from the self-play\n",
        "to the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GabAq2klI-ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alphazero(config: AlphaZeroConfig, network: Net):\n",
        "    replay_buffer = ReplayBuffer(config)\n",
        "\n",
        "    for i in range(config.cycles):\n",
        "        print(f\"self play {i} of {config.cycles}\")\n",
        "        network.eval()\n",
        "        run_selfplay(config, network, replay_buffer)\n",
        "        print(f\"train network {i} of {config.cycles}\")\n",
        "        network.train()\n",
        "        train_network(config, replay_buffer)\n",
        "\n",
        "    return network\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: AlphaZeroConfig, network: Net,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "    for i in range(config.window_size):  \n",
        "        if i % 10 == 0:\n",
        "            print(f\"game {i} of {config.window_size}\")\n",
        "        game = play_game(config, network)\n",
        "        replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: AlphaZeroConfig, network: Net):\n",
        "    game = Game()\n",
        "    while not game.terminal() and len(game.history) < config.max_moves:\n",
        "        action, root = run_mcts(config, game, network)\n",
        "        game.apply(action)\n",
        "        game.store_search_statistics(root)\n",
        "    return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: AlphaZeroConfig, game: Game, network: Net):\n",
        "    root = Node(0)\n",
        "    # Populate child nodes AKA the states that the actions available at this\n",
        "    # states would take you too\n",
        "    evaluate(root, game, network)\n",
        "    add_exploration_noise(config, root)\n",
        "\n",
        "    for i in range(config.num_simulations):\n",
        "        node = root\n",
        "        scratch_game = game.clone()\n",
        "        search_path = [node]\n",
        "\n",
        "        while node.expanded():\n",
        "            # Here we take one step down our search tree towards a win or loss. Note\n",
        "            # that we are resetting the node variable here to be the state that our\n",
        "            # game picked given the action we took.\n",
        "            #\n",
        "            # On the first run all child nodes will not be expanded, so we'll only\n",
        "            # take one step before backpropatagating back up the tree. This is a form\n",
        "            # of \"bootstrapping\"\n",
        "            action, node = select_child(config, node)\n",
        "            scratch_game.apply(action)\n",
        "            search_path.append(node)\n",
        "\n",
        "        value = evaluate(node, scratch_game, network)\n",
        "        backpropagate(search_path, value, scratch_game.to_play())\n",
        "    return select_action(config, game, root), root\n",
        "\n",
        "\n",
        "def select_action(config: AlphaZeroConfig, game: Game, root: Node):\n",
        "    visit_counts = [(child.visit_count, action)\n",
        "                    for action, child in iter(root.children.items())]\n",
        "    # TODO: does this even make sense for connect 4?\n",
        "    # if len(game.history) < config.num_sampling_moves:\n",
        "    #     _, action = softmax_sample(visit_counts)\n",
        "    # else:\n",
        "    _, action = max(visit_counts)\n",
        "    return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: AlphaZeroConfig, node: Node):\n",
        "    \"\"\"\n",
        "    Return the child node, i.e. action to take, that UCB likes best\n",
        "    \"\"\"\n",
        "    _, action, child = max((ucb_score(config, node, child), action, child)\n",
        "                           for action, child in iter(node.children.items()))\n",
        "    return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: AlphaZeroConfig, parent: Node, child: Node):\n",
        "    pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                    config.pb_c_base) + config.pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    prior_score = pb_c * child.prior\n",
        "    value_score = child.value()\n",
        "    return prior_score + value_score\n",
        "\n",
        "\n",
        "# We use the neural network to obtain a value and policy prediction.\n",
        "def evaluate(node: Node, game: Game, network: Net):\n",
        "    \"\"\"\n",
        "    Populate child nodes with priors and return value both derived from NN\n",
        "    Child nodes are the states that one could reach by taking the actions\n",
        "    available from the state that you are in.\n",
        "    \"\"\"\n",
        "    value, policy_logits = network.inference(game.make_image(len(game.history)))\n",
        "\n",
        "    # Expand the node.\n",
        "    node.to_play = game.to_play()\n",
        "    policy = {a: math.exp(policy_logits[a]) for a in game.legal_actions()}\n",
        "    policy_sum = sum(iter(policy.values()))\n",
        "    for action, p in iter(policy.items()):\n",
        "        # this is just softmax, notice the math.exp 3 lines up\n",
        "        node.children[action] = Node(p / policy_sum)\n",
        "    return value\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play):\n",
        "    for node in search_path:\n",
        "        node.value_sum += value if node.to_play == to_play else (1 - value)\n",
        "        node.visit_count += 1\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: AlphaZeroConfig, node: Node):\n",
        "    \"\"\"\n",
        "    Modifies the priors stored in nodes children with dirichlet noise whatever\n",
        "    that is\n",
        "    \"\"\"\n",
        "    actions = node.children.keys()\n",
        "    noise = numpy.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
        "    frac = config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "def train_network(config: AlphaZeroConfig, replay_buffer: ReplayBuffer):\n",
        "\n",
        "    # Weight decay takes care of our L2 regularization so it doesn't need to be in the loss function\n",
        "    optimizer = torch.optim.SGD(\n",
        "        network.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        momentum=config.momentum,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "\n",
        "    for i in range(config.training_steps): #(config.training_steps):\n",
        "        batch = replay_buffer.sample_batch()\n",
        "        update_weights(optimizer, network, batch, i+1)\n",
        "\n",
        "\n",
        "def update_weights(optimizer, network, batch, batch_num):\n",
        "\n",
        "\n",
        "    # Loop over each subset of data\n",
        "    for image, policy_target, value_target in batch:\n",
        "        # Zero out the optimizer's gradient buffer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make a prediction based on the model\n",
        "        policy, value = network(image)\n",
        "\n",
        "        # convert data to correct type\n",
        "        policy = policy.exp()\n",
        "        value = value.squeeze()\n",
        "\n",
        "\n",
        "        # Compute the loss\n",
        "        value_loss = nn.functional.mse_loss(value, value_target.to(device))\n",
        "        policy_loss = nn.functional.binary_cross_entropy(policy, policy_target.to(device))\n",
        "        loss = value_loss + policy_loss\n",
        "\n",
        "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Use the derivative information to update the parameters\n",
        "        optimizer.step()\n",
        "    print(\"Batch: %d    Loss: %f\" % (batch_num, loss))\n",
        "def make_uniform_network():\n",
        "    network = Net()\n",
        "    network.float()\n",
        "    return network\n",
        "\n",
        "\n",
        "def interactive_game(config: AlphaZeroConfig, network: Net):\n",
        "\n",
        "    play_again = 'y'\n",
        "    while play_again == 'y':\n",
        "        game = Game()\n",
        "        print(game)\n",
        "        while not game.terminal():\n",
        "            while True:\n",
        "                print(\"choose move please: \", end='')\n",
        "                human_action = input()\n",
        "                try:\n",
        "                    if int(human_action) not in game.legal_actions():\n",
        "                        print(\"illegal action\")\n",
        "                    else:\n",
        "                        break\n",
        "                except ValueError:\n",
        "                    print(\"illegal action\")\n",
        "            game.apply(int(human_action))\n",
        "            # print(game)\n",
        "            ai_action, _ = run_mcts(config, game, network)\n",
        "            print(f\"ai chooses {ai_action}\")\n",
        "            game.apply(ai_action)\n",
        "            print(game)\n",
        "        win_string = {-1: \"lost\", 1: \"won\", 0: \"tied\"}\n",
        "        print(f\"you {win_string[game.terminal_value(0)]}\")\n",
        "        print(game)\n",
        "        while True:\n",
        "            print(\"Play again? y or n?\", end='')\n",
        "            play_again = input()\n",
        "            try:\n",
        "                if play_again != 'y' and play_again != 'n':\n",
        "                    print(\"I didn't understand\")\n",
        "                else:\n",
        "                    break\n",
        "            except ValueError:\n",
        "                print(\"illegal action\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TxQRHX2qI-ue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37b7c6d3-174c-4ff7-aedd-b7359205d4dd"
      },
      "source": [
        "\n",
        "print(\"Device: %s\" % device)\n",
        "network = make_uniform_network()\n",
        "network = Net().to(device)\n",
        "config = AlphaZeroConfig()\n",
        "# interactive_game(config, network)\n",
        "alphazero(config, network)\n",
        "interactive_game(config, network)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n",
            "self play 0 of 3\n",
            "game 0 of 20\n",
            "game 10 of 20\n",
            "train network 0 of 3\n",
            "Batch: 1    Loss: 0.514468\n",
            "Batch: 2    Loss: 0.429443\n",
            "Batch: 3    Loss: 0.410699\n",
            "Batch: 4    Loss: 0.400144\n",
            "Batch: 5    Loss: 0.405353\n",
            "Batch: 6    Loss: 0.310677\n",
            "Batch: 7    Loss: 0.265007\n",
            "Batch: 8    Loss: 0.268451\n",
            "Batch: 9    Loss: 0.239113\n",
            "self play 1 of 3\n",
            "game 0 of 20\n",
            "game 10 of 20\n",
            "train network 1 of 3\n",
            "Batch: 1    Loss: 0.374879\n",
            "Batch: 2    Loss: 0.396130\n",
            "Batch: 3    Loss: 0.350048\n",
            "Batch: 4    Loss: 0.345487\n",
            "Batch: 5    Loss: 0.348624\n",
            "Batch: 6    Loss: 0.360704\n",
            "Batch: 7    Loss: 0.323100\n",
            "Batch: 8    Loss: 0.345978\n",
            "Batch: 9    Loss: 0.311666\n",
            "self play 2 of 3\n",
            "game 0 of 20\n",
            "game 10 of 20\n",
            "train network 2 of 3\n",
            "Batch: 1    Loss: 0.486877\n",
            "Batch: 2    Loss: 0.406004\n",
            "Batch: 3    Loss: 0.357086\n",
            "Batch: 4    Loss: 0.348292\n",
            "Batch: 5    Loss: 0.337701\n",
            "Batch: 6    Loss: 0.340497\n",
            "Batch: 7    Loss: 0.331145\n",
            "Batch: 8    Loss: 0.342436\n",
            "Batch: 9    Loss: 0.345595\n",
            "0|                     |\n",
            "1|                     |\n",
            "2|                     |\n",
            "3|                     |\n",
            "4|                     |\n",
            "5|                     |\n",
            "   ̅0  ̅1  ̅2  ̅3  ̅4  ̅5  ̅6 \n",
            "choose move please: ai chooses 1\n",
            "0|                     |\n",
            "1|                     |\n",
            "2|                     |\n",
            "3|                     |\n",
            "4|                     |\n",
            "5| ○  ●                |\n",
            "   ̅0  ̅1  ̅2  ̅3  ̅4  ̅5  ̅6 \n",
            "choose move please: ai chooses 0\n",
            "0|                     |\n",
            "1|                     |\n",
            "2|                     |\n",
            "3| ●                   |\n",
            "4| ○                   |\n",
            "5| ○  ●                |\n",
            "   ̅0  ̅1  ̅2  ̅3  ̅4  ̅5  ̅6 \n",
            "choose move please: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-488923aa4fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# interactive_game(config, network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0malphazero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minteractive_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-691fada69488>\u001b[0m in \u001b[0;36minteractive_game\u001b[0;34m(config, network)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"choose move please: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mhuman_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_action\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iew2t2IpJL_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}